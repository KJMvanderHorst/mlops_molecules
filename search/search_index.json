{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#documentation","title":"Documentation","text":"<p>Documentation for project_name</p>"},{"location":"data/","title":"Data and I/O","text":"<p>Data handling utilities and model artifact download helpers.</p>"},{"location":"data/#data-module","title":"Data module","text":""},{"location":"data/#project_name.data","title":"project_name.data","text":""},{"location":"data/#project_name.data.QM9Dataset","title":"QM9Dataset","text":"<p>               Bases: <code>Dataset</code></p> <p>QM9 dataset wrapper from torch_geometric.</p> Source code in <code>src/project_name/data.py</code> <pre><code>class QM9Dataset(Dataset):\n    \"\"\"QM9 dataset wrapper from torch_geometric.\"\"\"\n\n    def __init__(self, data_path: Path) -&gt; None:\n        \"\"\"Initialize the QM9 dataset.\n\n        Args:\n            data_path: Path to the data directory where QM9 will be stored.\n        \"\"\"\n        self.data_path = Path(data_path)\n        self.dataset = self._load_dataset()\n\n    def _load_dataset(self) -&gt; QM9:\n        \"\"\"Load QM9 dataset, checking if it already exists locally.\n\n        Downloads the dataset on first instantiation if it doesn't exist.\n\n        Returns:\n            QM9 dataset from torch_geometric.\n        \"\"\"\n        raw_path = self.data_path / \"raw\"\n        raw_path.mkdir(parents=True, exist_ok=True)\n\n        print(\"Loading QM9 dataset (downloading if not already present)...\")\n        dataset = QM9(root=str(self.data_path))\n        print(f\"Dataset ready at {self.data_path}\")\n        return dataset\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.dataset)\n\n    def __getitem__(self, index: int):\n        \"\"\"Return a given sample from the dataset.\"\"\"\n        return self.dataset[index]\n\n    def preprocess(self, output_folder: Path) -&gt; None:\n        \"\"\"Preprocess the raw data and save it to the output folder.\"\"\"\n</code></pre>"},{"location":"data/#project_name.data.QM9Dataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: int)\n</code></pre> <p>Return a given sample from the dataset.</p> Source code in <code>src/project_name/data.py</code> <pre><code>def __getitem__(self, index: int):\n    \"\"\"Return a given sample from the dataset.\"\"\"\n    return self.dataset[index]\n</code></pre>"},{"location":"data/#project_name.data.QM9Dataset.__init__","title":"__init__","text":"<pre><code>__init__(data_path: Path) -&gt; None\n</code></pre> <p>Initialize the QM9 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>Path to the data directory where QM9 will be stored.</p> required Source code in <code>src/project_name/data.py</code> <pre><code>def __init__(self, data_path: Path) -&gt; None:\n    \"\"\"Initialize the QM9 dataset.\n\n    Args:\n        data_path: Path to the data directory where QM9 will be stored.\n    \"\"\"\n    self.data_path = Path(data_path)\n    self.dataset = self._load_dataset()\n</code></pre>"},{"location":"data/#project_name.data.QM9Dataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>src/project_name/data.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"data/#project_name.data.QM9Dataset.preprocess","title":"preprocess","text":"<pre><code>preprocess(output_folder: Path) -&gt; None\n</code></pre> <p>Preprocess the raw data and save it to the output folder.</p> Source code in <code>src/project_name/data.py</code> <pre><code>def preprocess(self, output_folder: Path) -&gt; None:\n    \"\"\"Preprocess the raw data and save it to the output folder.\"\"\"\n</code></pre>"},{"location":"data/#model-download","title":"Model download","text":""},{"location":"data/#project_name.download_model","title":"project_name.download_model","text":""},{"location":"model/","title":"Model","text":"<p>Graph neural network architecture for molecular property regression.</p>"},{"location":"model/#module","title":"Module","text":""},{"location":"model/#project_name.model","title":"project_name.model","text":""},{"location":"model/#project_name.model.GraphNeuralNetwork","title":"GraphNeuralNetwork","text":"<p>               Bases: <code>Module</code></p> <p>Graph Neural Network for molecular property regression.</p> Source code in <code>src/project_name/model.py</code> <pre><code>class GraphNeuralNetwork(nn.Module):\n    \"\"\"Graph Neural Network for molecular property regression.\"\"\"\n\n    def __init__(\n        self,\n        num_node_features: int = 11,\n        num_edge_features: int = 4,\n        hidden_dim: int = 128,\n        num_layers: int = 3,\n        output_dim: int = 1,\n        dropout: float = 0.1,\n    ) -&gt; None:\n        \"\"\"Initialize the GNN model.\n\n        Args:\n            num_node_features: Number of node (atom) features.\n            num_edge_features: Number of edge (bond) features.\n            hidden_dim: Number of hidden channels.\n            num_layers: Number of GraphConv layers.\n\n\n            output_dim: Output dimension (1 for single property regression).\n            dropout: Dropout rate for regularization.\n        \"\"\"\n        super().__init__()\n        self.dropout_rate = dropout\n\n        self.initial_embedding = nn.Linear(num_node_features, hidden_dim)\n\n        self.conv_layers = nn.ModuleList([GraphConv(hidden_dim, hidden_dim) for _ in range(num_layers)])\n\n        self.pool = global_mean_pool\n\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, output_dim),\n        )\n\n    def forward(self, data) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the model.\n\n        Args:\n            data: PyTorch Geometric Data object with x, edge_index, and batch attributes.\n\n        Returns:\n            Predicted property values.\n        \"\"\"\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n\n        x = self.initial_embedding(x)\n        x = F.relu(x)\n\n        for conv in self.conv_layers:\n            x = conv(x, edge_index)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n\n        x = self.pool(x, batch)\n\n        x = self.mlp(x)\n\n        return x\n</code></pre>"},{"location":"model/#project_name.model.GraphNeuralNetwork.__init__","title":"__init__","text":"<pre><code>__init__(num_node_features: int = 11, num_edge_features: int = 4, hidden_dim: int = 128, num_layers: int = 3, output_dim: int = 1, dropout: float = 0.1) -&gt; None\n</code></pre> <p>Initialize the GNN model.</p> <p>Parameters:</p> Name Type Description Default <code>num_node_features</code> <code>int</code> <p>Number of node (atom) features.</p> <code>11</code> <code>num_edge_features</code> <code>int</code> <p>Number of edge (bond) features.</p> <code>4</code> <code>hidden_dim</code> <code>int</code> <p>Number of hidden channels.</p> <code>128</code> <code>num_layers</code> <code>int</code> <p>Number of GraphConv layers.</p> <code>3</code> <code>output_dim</code> <code>int</code> <p>Output dimension (1 for single property regression).</p> <code>1</code> <code>dropout</code> <code>float</code> <p>Dropout rate for regularization.</p> <code>0.1</code> Source code in <code>src/project_name/model.py</code> <pre><code>def __init__(\n    self,\n    num_node_features: int = 11,\n    num_edge_features: int = 4,\n    hidden_dim: int = 128,\n    num_layers: int = 3,\n    output_dim: int = 1,\n    dropout: float = 0.1,\n) -&gt; None:\n    \"\"\"Initialize the GNN model.\n\n    Args:\n        num_node_features: Number of node (atom) features.\n        num_edge_features: Number of edge (bond) features.\n        hidden_dim: Number of hidden channels.\n        num_layers: Number of GraphConv layers.\n\n\n        output_dim: Output dimension (1 for single property regression).\n        dropout: Dropout rate for regularization.\n    \"\"\"\n    super().__init__()\n    self.dropout_rate = dropout\n\n    self.initial_embedding = nn.Linear(num_node_features, hidden_dim)\n\n    self.conv_layers = nn.ModuleList([GraphConv(hidden_dim, hidden_dim) for _ in range(num_layers)])\n\n    self.pool = global_mean_pool\n\n    self.mlp = nn.Sequential(\n        nn.Linear(hidden_dim, hidden_dim),\n        nn.ReLU(),\n        nn.Dropout(dropout),\n        nn.Linear(hidden_dim, hidden_dim // 2),\n        nn.ReLU(),\n        nn.Dropout(dropout),\n        nn.Linear(hidden_dim // 2, output_dim),\n    )\n</code></pre>"},{"location":"model/#project_name.model.GraphNeuralNetwork.forward","title":"forward","text":"<pre><code>forward(data) -&gt; torch.Tensor\n</code></pre> <p>Forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>PyTorch Geometric Data object with x, edge_index, and batch attributes.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Predicted property values.</p> Source code in <code>src/project_name/model.py</code> <pre><code>def forward(self, data) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the model.\n\n    Args:\n        data: PyTorch Geometric Data object with x, edge_index, and batch attributes.\n\n    Returns:\n        Predicted property values.\n    \"\"\"\n    x, edge_index, batch = data.x, data.edge_index, data.batch\n\n    x = self.initial_embedding(x)\n    x = F.relu(x)\n\n    for conv in self.conv_layers:\n        x = conv(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n\n    x = self.pool(x, batch)\n\n    x = self.mlp(x)\n\n    return x\n</code></pre>"},{"location":"my_api/","title":"Inference API","text":"<p>The FastAPI service wraps the graph neural network to expose prediction endpoints.</p>"},{"location":"my_api/#inference-service","title":"Inference Service","text":""},{"location":"my_api/#project_name.api.InferenceService","title":"project_name.api.InferenceService","text":"<p>Handles model loading and predictions.</p> Source code in <code>src/project_name/api.py</code> <pre><code>class InferenceService:\n    \"\"\"Handles model loading and predictions.\"\"\"\n\n    def __init__(self, model_path: str | Path) -&gt; None:\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = GraphNeuralNetwork(\n            num_node_features=11,\n            hidden_dim=128,\n            num_layers=3,\n            output_dim=1,\n        )\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()\n\n    def predict(\n        self,\n        node_features: list[list[float]],\n        edge_index: list[list[int]],\n    ) -&gt; list[float]:\n        \"\"\"Generate prediction.\"\"\"\n        with torch.no_grad():\n            x = torch.tensor(node_features, dtype=torch.float32).to(self.device)\n            edge_idx = torch.tensor(edge_index, dtype=torch.long).to(self.device)\n            data = Data(x=x, edge_index=edge_idx)\n            output = self.model(data)\n            return [float(output.squeeze().cpu())]\n</code></pre>"},{"location":"my_api/#project_name.api.InferenceService.predict","title":"predict","text":"<pre><code>predict(node_features: list[list[float]], edge_index: list[list[int]]) -&gt; list[float]\n</code></pre> <p>Generate prediction.</p> Source code in <code>src/project_name/api.py</code> <pre><code>def predict(\n    self,\n    node_features: list[list[float]],\n    edge_index: list[list[int]],\n) -&gt; list[float]:\n    \"\"\"Generate prediction.\"\"\"\n    with torch.no_grad():\n        x = torch.tensor(node_features, dtype=torch.float32).to(self.device)\n        edge_idx = torch.tensor(edge_index, dtype=torch.long).to(self.device)\n        data = Data(x=x, edge_index=edge_idx)\n        output = self.model(data)\n        return [float(output.squeeze().cpu())]\n</code></pre>"},{"location":"my_api/#requestresponse-schemas","title":"Request/Response Schemas","text":""},{"location":"my_api/#project_name.api.PredictionRequest","title":"project_name.api.PredictionRequest","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input for prediction.</p> Source code in <code>src/project_name/api.py</code> <pre><code>class PredictionRequest(BaseModel):\n    \"\"\"Input for prediction.\"\"\"\n\n    node_features: list[list[float]]\n    edge_index: list[list[int]]\n\n    @field_validator(\"node_features\")\n    def validate_node_features(cls, v: list[list[float]]) -&gt; list[list[float]]:\n        \"\"\"Validate node features have correct number of features.\n\n        Args:\n            v: Node features matrix.\n\n        Returns:\n            Validated node features.\n\n        Raises:\n            ValueError: If node features don't have correct number of dimensions.\n        \"\"\"\n        if not v:\n            raise ValueError(\"node_features cannot be empty\")\n        num_features = len(v[0])\n        if num_features != 11:\n            raise ValueError(f\"Each node must have exactly 11 features, got {num_features}\")\n        if not all(len(features) == num_features for features in v):\n            raise ValueError(\"All nodes must have the same number of features\")\n        return v\n</code></pre>"},{"location":"my_api/#project_name.api.PredictionRequest.validate_node_features","title":"validate_node_features","text":"<pre><code>validate_node_features(v: list[list[float]]) -&gt; list[list[float]]\n</code></pre> <p>Validate node features have correct number of features.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>list[list[float]]</code> <p>Node features matrix.</p> required <p>Returns:</p> Type Description <code>list[list[float]]</code> <p>Validated node features.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If node features don't have correct number of dimensions.</p> Source code in <code>src/project_name/api.py</code> <pre><code>@field_validator(\"node_features\")\ndef validate_node_features(cls, v: list[list[float]]) -&gt; list[list[float]]:\n    \"\"\"Validate node features have correct number of features.\n\n    Args:\n        v: Node features matrix.\n\n    Returns:\n        Validated node features.\n\n    Raises:\n        ValueError: If node features don't have correct number of dimensions.\n    \"\"\"\n    if not v:\n        raise ValueError(\"node_features cannot be empty\")\n    num_features = len(v[0])\n    if num_features != 11:\n        raise ValueError(f\"Each node must have exactly 11 features, got {num_features}\")\n    if not all(len(features) == num_features for features in v):\n        raise ValueError(\"All nodes must have the same number of features\")\n    return v\n</code></pre>"},{"location":"my_api/#project_name.api.PredictionResponse","title":"project_name.api.PredictionResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Prediction output.</p> Source code in <code>src/project_name/api.py</code> <pre><code>class PredictionResponse(BaseModel):\n    \"\"\"Prediction output.\"\"\"\n\n    prediction: list[float]\n</code></pre>"},{"location":"my_api/#routes","title":"Routes","text":""},{"location":"my_api/#project_name.api.health_check","title":"project_name.api.health_check","text":"<pre><code>health_check()\n</code></pre> <p>Health check.</p> Source code in <code>src/project_name/api.py</code> <pre><code>@app.get(\"/\")\ndef health_check():\n    \"\"\"Health check.\"\"\"\n    return {\"status\": \"healthy\"}\n</code></pre>"},{"location":"my_api/#project_name.api.predict","title":"project_name.api.predict","text":"<pre><code>predict(request: PredictionRequest, background_tasks: BackgroundTasks)\n</code></pre> <p>Generate prediction.</p> Source code in <code>src/project_name/api.py</code> <pre><code>@app.post(\"/predict\", response_model=PredictionResponse)\ndef predict(request: PredictionRequest, background_tasks: BackgroundTasks):\n    \"\"\"Generate prediction.\"\"\"\n    if service is None:\n        raise HTTPException(status_code=503, detail=\"Model not ready\")\n\n    try:\n        prediction = service.predict(\n            request.node_features,\n            request.edge_index,\n        )\n        background_tasks.add_task(save_prediction_to_gcp, request.node_features, request.edge_index, prediction)\n        return PredictionResponse(prediction=prediction)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre>"},{"location":"my_api/#modules","title":"Modules","text":""},{"location":"my_api/#api","title":"api","text":""},{"location":"my_api/#project_name.api","title":"project_name.api","text":""},{"location":"my_api/#project_name.api.InferenceService","title":"InferenceService","text":"<p>Handles model loading and predictions.</p> Source code in <code>src/project_name/api.py</code> <pre><code>class InferenceService:\n    \"\"\"Handles model loading and predictions.\"\"\"\n\n    def __init__(self, model_path: str | Path) -&gt; None:\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = GraphNeuralNetwork(\n            num_node_features=11,\n            hidden_dim=128,\n            num_layers=3,\n            output_dim=1,\n        )\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()\n\n    def predict(\n        self,\n        node_features: list[list[float]],\n        edge_index: list[list[int]],\n    ) -&gt; list[float]:\n        \"\"\"Generate prediction.\"\"\"\n        with torch.no_grad():\n            x = torch.tensor(node_features, dtype=torch.float32).to(self.device)\n            edge_idx = torch.tensor(edge_index, dtype=torch.long).to(self.device)\n            data = Data(x=x, edge_index=edge_idx)\n            output = self.model(data)\n            return [float(output.squeeze().cpu())]\n</code></pre>"},{"location":"my_api/#project_name.api.InferenceService.predict","title":"predict","text":"<pre><code>predict(node_features: list[list[float]], edge_index: list[list[int]]) -&gt; list[float]\n</code></pre> <p>Generate prediction.</p> Source code in <code>src/project_name/api.py</code> <pre><code>def predict(\n    self,\n    node_features: list[list[float]],\n    edge_index: list[list[int]],\n) -&gt; list[float]:\n    \"\"\"Generate prediction.\"\"\"\n    with torch.no_grad():\n        x = torch.tensor(node_features, dtype=torch.float32).to(self.device)\n        edge_idx = torch.tensor(edge_index, dtype=torch.long).to(self.device)\n        data = Data(x=x, edge_index=edge_idx)\n        output = self.model(data)\n        return [float(output.squeeze().cpu())]\n</code></pre>"},{"location":"my_api/#project_name.api.PredictionRequest","title":"PredictionRequest","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input for prediction.</p> Source code in <code>src/project_name/api.py</code> <pre><code>class PredictionRequest(BaseModel):\n    \"\"\"Input for prediction.\"\"\"\n\n    node_features: list[list[float]]\n    edge_index: list[list[int]]\n\n    @field_validator(\"node_features\")\n    def validate_node_features(cls, v: list[list[float]]) -&gt; list[list[float]]:\n        \"\"\"Validate node features have correct number of features.\n\n        Args:\n            v: Node features matrix.\n\n        Returns:\n            Validated node features.\n\n        Raises:\n            ValueError: If node features don't have correct number of dimensions.\n        \"\"\"\n        if not v:\n            raise ValueError(\"node_features cannot be empty\")\n        num_features = len(v[0])\n        if num_features != 11:\n            raise ValueError(f\"Each node must have exactly 11 features, got {num_features}\")\n        if not all(len(features) == num_features for features in v):\n            raise ValueError(\"All nodes must have the same number of features\")\n        return v\n</code></pre>"},{"location":"my_api/#project_name.api.PredictionRequest.validate_node_features","title":"validate_node_features","text":"<pre><code>validate_node_features(v: list[list[float]]) -&gt; list[list[float]]\n</code></pre> <p>Validate node features have correct number of features.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>list[list[float]]</code> <p>Node features matrix.</p> required <p>Returns:</p> Type Description <code>list[list[float]]</code> <p>Validated node features.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If node features don't have correct number of dimensions.</p> Source code in <code>src/project_name/api.py</code> <pre><code>@field_validator(\"node_features\")\ndef validate_node_features(cls, v: list[list[float]]) -&gt; list[list[float]]:\n    \"\"\"Validate node features have correct number of features.\n\n    Args:\n        v: Node features matrix.\n\n    Returns:\n        Validated node features.\n\n    Raises:\n        ValueError: If node features don't have correct number of dimensions.\n    \"\"\"\n    if not v:\n        raise ValueError(\"node_features cannot be empty\")\n    num_features = len(v[0])\n    if num_features != 11:\n        raise ValueError(f\"Each node must have exactly 11 features, got {num_features}\")\n    if not all(len(features) == num_features for features in v):\n        raise ValueError(\"All nodes must have the same number of features\")\n    return v\n</code></pre>"},{"location":"my_api/#project_name.api.PredictionResponse","title":"PredictionResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Prediction output.</p> Source code in <code>src/project_name/api.py</code> <pre><code>class PredictionResponse(BaseModel):\n    \"\"\"Prediction output.\"\"\"\n\n    prediction: list[float]\n</code></pre>"},{"location":"my_api/#project_name.api.health_check","title":"health_check","text":"<pre><code>health_check()\n</code></pre> <p>Health check.</p> Source code in <code>src/project_name/api.py</code> <pre><code>@app.get(\"/\")\ndef health_check():\n    \"\"\"Health check.\"\"\"\n    return {\"status\": \"healthy\"}\n</code></pre>"},{"location":"my_api/#project_name.api.lifespan","title":"lifespan  <code>async</code>","text":"<pre><code>lifespan(app: FastAPI)\n</code></pre> <p>Load model on startup.</p> Source code in <code>src/project_name/api.py</code> <pre><code>@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Load model on startup.\"\"\"\n    global service\n    model_path = \"best_model.pt\"\n    service = InferenceService(MODEL_FOLDER + model_path)\n\n    yield\n\n    del service\n</code></pre>"},{"location":"my_api/#project_name.api.predict","title":"predict","text":"<pre><code>predict(request: PredictionRequest, background_tasks: BackgroundTasks)\n</code></pre> <p>Generate prediction.</p> Source code in <code>src/project_name/api.py</code> <pre><code>@app.post(\"/predict\", response_model=PredictionResponse)\ndef predict(request: PredictionRequest, background_tasks: BackgroundTasks):\n    \"\"\"Generate prediction.\"\"\"\n    if service is None:\n        raise HTTPException(status_code=503, detail=\"Model not ready\")\n\n    try:\n        prediction = service.predict(\n            request.node_features,\n            request.edge_index,\n        )\n        background_tasks.add_task(save_prediction_to_gcp, request.node_features, request.edge_index, prediction)\n        return PredictionResponse(prediction=prediction)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre>"},{"location":"my_api/#project_name.api.save_prediction_to_gcp","title":"save_prediction_to_gcp","text":"<pre><code>save_prediction_to_gcp(node_features: list[list[float]], edge_index: list[list[int]], outputs: list[float])\n</code></pre> <p>Save the prediction results to GCP bucket.</p> Source code in <code>src/project_name/api.py</code> <pre><code>def save_prediction_to_gcp(node_features: list[list[float]], edge_index: list[list[int]], outputs: list[float]):\n    \"\"\"Save the prediction results to GCP bucket.\"\"\"\n    client = storage.Client()\n    bucket = client.bucket(PRED_FOLDER)\n    time = datetime.now(tz=timezone.utc).isoformat()\n    # Prepare prediction data\n    data = {\n        \"node_features\": node_features,\n        \"edge_index\": edge_index,\n        \"prediction\": outputs,\n        \"timestamp\": time,\n    }\n    blob = bucket.blob(f\"predictions/{time}.json\")\n    blob.upload_from_string(json.dumps(data))\n    print(\"Prediction saved to GCP bucket.\")\n</code></pre>"},{"location":"my_api/#compare_promote","title":"compare_promote","text":""},{"location":"my_api/#project_name.compare_promote","title":"project_name.compare_promote","text":""},{"location":"my_api/#data","title":"data","text":""},{"location":"my_api/#project_name.data","title":"project_name.data","text":""},{"location":"my_api/#project_name.data.QM9Dataset","title":"QM9Dataset","text":"<p>               Bases: <code>Dataset</code></p> <p>QM9 dataset wrapper from torch_geometric.</p> Source code in <code>src/project_name/data.py</code> <pre><code>class QM9Dataset(Dataset):\n    \"\"\"QM9 dataset wrapper from torch_geometric.\"\"\"\n\n    def __init__(self, data_path: Path) -&gt; None:\n        \"\"\"Initialize the QM9 dataset.\n\n        Args:\n            data_path: Path to the data directory where QM9 will be stored.\n        \"\"\"\n        self.data_path = Path(data_path)\n        self.dataset = self._load_dataset()\n\n    def _load_dataset(self) -&gt; QM9:\n        \"\"\"Load QM9 dataset, checking if it already exists locally.\n\n        Downloads the dataset on first instantiation if it doesn't exist.\n\n        Returns:\n            QM9 dataset from torch_geometric.\n        \"\"\"\n        raw_path = self.data_path / \"raw\"\n        raw_path.mkdir(parents=True, exist_ok=True)\n\n        print(\"Loading QM9 dataset (downloading if not already present)...\")\n        dataset = QM9(root=str(self.data_path))\n        print(f\"Dataset ready at {self.data_path}\")\n        return dataset\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.dataset)\n\n    def __getitem__(self, index: int):\n        \"\"\"Return a given sample from the dataset.\"\"\"\n        return self.dataset[index]\n\n    def preprocess(self, output_folder: Path) -&gt; None:\n        \"\"\"Preprocess the raw data and save it to the output folder.\"\"\"\n</code></pre>"},{"location":"my_api/#project_name.data.QM9Dataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: int)\n</code></pre> <p>Return a given sample from the dataset.</p> Source code in <code>src/project_name/data.py</code> <pre><code>def __getitem__(self, index: int):\n    \"\"\"Return a given sample from the dataset.\"\"\"\n    return self.dataset[index]\n</code></pre>"},{"location":"my_api/#project_name.data.QM9Dataset.__init__","title":"__init__","text":"<pre><code>__init__(data_path: Path) -&gt; None\n</code></pre> <p>Initialize the QM9 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>Path to the data directory where QM9 will be stored.</p> required Source code in <code>src/project_name/data.py</code> <pre><code>def __init__(self, data_path: Path) -&gt; None:\n    \"\"\"Initialize the QM9 dataset.\n\n    Args:\n        data_path: Path to the data directory where QM9 will be stored.\n    \"\"\"\n    self.data_path = Path(data_path)\n    self.dataset = self._load_dataset()\n</code></pre>"},{"location":"my_api/#project_name.data.QM9Dataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return the length of the dataset.</p> Source code in <code>src/project_name/data.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"my_api/#project_name.data.QM9Dataset.preprocess","title":"preprocess","text":"<pre><code>preprocess(output_folder: Path) -&gt; None\n</code></pre> <p>Preprocess the raw data and save it to the output folder.</p> Source code in <code>src/project_name/data.py</code> <pre><code>def preprocess(self, output_folder: Path) -&gt; None:\n    \"\"\"Preprocess the raw data and save it to the output folder.\"\"\"\n</code></pre>"},{"location":"my_api/#download_model","title":"download_model","text":""},{"location":"my_api/#project_name.download_model","title":"project_name.download_model","text":""},{"location":"my_api/#evaluate","title":"evaluate","text":""},{"location":"my_api/#project_name.evaluate","title":"project_name.evaluate","text":""},{"location":"my_api/#project_name.evaluate.evaluate","title":"evaluate","text":"<pre><code>evaluate(model: GraphNeuralNetwork, loader: DataLoader, device: torch.device, target_indices: Sequence[int]) -&gt; float\n</code></pre> <p>Evaluate model on a dataloader.</p> <p>Computes mean MSE loss per graph over the entire loader, matching train_epoch.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GraphNeuralNetwork</code> <p>Trained GNN model.</p> required <code>loader</code> <code>DataLoader</code> <p>DataLoader for validation/test set.</p> required <code>device</code> <code>device</code> <p>Torch device.</p> required <code>target_indices</code> <code>Sequence[int]</code> <p>Indices of target properties in batch.y.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean MSE loss per graph.</p> Source code in <code>src/project_name/evaluate.py</code> <pre><code>@torch.no_grad()\ndef evaluate(\n    model: GraphNeuralNetwork,\n    loader: DataLoader,\n    device: torch.device,\n    target_indices: Sequence[int],\n) -&gt; float:\n    \"\"\"Evaluate model on a dataloader.\n\n    Computes mean MSE loss per graph over the entire loader, matching train_epoch.\n\n    Args:\n        model: Trained GNN model.\n        loader: DataLoader for validation/test set.\n        device: Torch device.\n        target_indices: Indices of target properties in batch.y.\n\n    Returns:\n        Mean MSE loss per graph.\n    \"\"\"\n    model.eval()\n\n    total_loss: float = 0.0\n    num_samples: int = 0\n\n    target_idx = list(target_indices)\n\n    for batch in loader:\n        batch = batch.to(device)\n\n        pred: torch.Tensor = model(batch)\n        target: torch.Tensor = batch.y[:, target_idx]\n\n        loss: torch.Tensor = F.mse_loss(pred, target)\n        total_loss += loss.item() * batch.num_graphs\n        num_samples += batch.num_graphs\n\n    if num_samples == 0:\n        return 0.0\n\n    return total_loss / num_samples\n</code></pre>"},{"location":"my_api/#project_name.evaluate.evaluate_with_metrics","title":"evaluate_with_metrics","text":"<pre><code>evaluate_with_metrics(model: GraphNeuralNetwork, loader: DataLoader, device: torch.device, target_indices: Sequence[int]) -&gt; dict[str, float]\n</code></pre> <p>Evaluate model on a dataloader with multiple metrics.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GraphNeuralNetwork</code> <p>Trained GNN model.</p> required <code>loader</code> <code>DataLoader</code> <p>DataLoader for validation/test set.</p> required <code>device</code> <code>device</code> <p>Torch device.</p> required <code>target_indices</code> <code>Sequence[int]</code> <p>Indices of target properties in batch.y.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with metrics: mse, rmse, mae, r2.</p> Source code in <code>src/project_name/evaluate.py</code> <pre><code>@torch.no_grad()\ndef evaluate_with_metrics(\n    model: GraphNeuralNetwork,\n    loader: DataLoader,\n    device: torch.device,\n    target_indices: Sequence[int],\n) -&gt; dict[str, float]:\n    \"\"\"Evaluate model on a dataloader with multiple metrics.\n\n    Args:\n        model: Trained GNN model.\n        loader: DataLoader for validation/test set.\n        device: Torch device.\n        target_indices: Indices of target properties in batch.y.\n\n    Returns:\n        Dictionary with metrics: mse, rmse, mae, r2.\n    \"\"\"\n    model.eval()\n\n    all_preds: list[torch.Tensor] = []\n    all_targets: list[torch.Tensor] = []\n\n    target_idx = list(target_indices)\n\n    for batch in loader:\n        batch = batch.to(device)\n\n        pred: torch.Tensor = model(batch)\n        target: torch.Tensor = batch.y[:, target_idx]\n\n        all_preds.append(pred)\n        all_targets.append(target)\n\n    if len(all_preds) == 0:\n        return {\"mse\": 0.0, \"rmse\": 0.0, \"mae\": 0.0, \"r2\": 0.0}\n\n    predictions = torch.cat(all_preds, dim=0)\n    targets = torch.cat(all_targets, dim=0)\n\n    # MSE\n    mse = F.mse_loss(predictions, targets).item()\n\n    # RMSE\n    rmse = torch.sqrt(F.mse_loss(predictions, targets)).item()\n\n    # MAE\n    mae = F.l1_loss(predictions, targets).item()\n\n    # R\u00b2 score\n    ss_res = torch.sum((targets - predictions) ** 2).item()\n    ss_tot = torch.sum((targets - torch.mean(targets)) ** 2).item()\n    r2 = 1 - (ss_res / ss_tot) if ss_tot &gt; 0 else 0.0\n\n    return {\n        \"mse\": mse,\n        \"rmse\": rmse,\n        \"mae\": mae,\n        \"r2\": r2,\n    }\n</code></pre>"},{"location":"my_api/#project_name.evaluate.get_device","title":"get_device","text":"<pre><code>get_device() -&gt; torch.device\n</code></pre> <p>Get the best available device for computation.</p> Source code in <code>src/project_name/evaluate.py</code> <pre><code>def get_device() -&gt; torch.device:\n    \"\"\"Get the best available device for computation.\"\"\"\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    if torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    return torch.device(\"cpu\")\n</code></pre>"},{"location":"my_api/#project_name.evaluate.main","title":"main","text":"<pre><code>main(cfg: DictConfig) -&gt; None\n</code></pre> <p>Load best model and evaluate on test set with comprehensive metrics.</p> Source code in <code>src/project_name/evaluate.py</code> <pre><code>@hydra.main(version_base=None, config_path=\"../../configs\", config_name=\"config\")\ndef main(cfg: DictConfig) -&gt; None:\n    \"\"\"Load best model and evaluate on test set with comprehensive metrics.\"\"\"\n    device = get_device()\n\n    # Load dataset\n    dataset = QM9Dataset(cfg.training.data_path)\n    dataset.transform = NormalizeScale()\n\n    n = len(dataset)\n    train_size = int(cfg.training.train_ratio * n)\n    val_size = int(cfg.training.val_ratio * n)\n    test_size = n - train_size - val_size\n\n    _, _, test_dataset = torch.utils.data.random_split(\n        dataset,\n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(cfg.seed),\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=cfg.training.batch_size,\n        shuffle=False,\n    )\n\n    target_indices = list(cfg.training.target_indices)\n    num_targets = len(target_indices)\n\n    # Build model\n    model = GraphNeuralNetwork(\n        num_node_features=cfg.model.num_node_features,\n        hidden_dim=cfg.model.hidden_dim,\n        num_layers=cfg.model.num_layers,\n        output_dim=num_targets,\n    ).to(device)\n\n    # Load best model\n    best_model_path = Path(cfg.training.model_dir) / \"best_model.pt\"\n    print(f\"Loading model from: {best_model_path}\")\n\n    try:\n        state = torch.load(best_model_path, weights_only=True)\n    except TypeError:\n        state = torch.load(best_model_path)\n\n    model.load_state_dict(state)\n\n    # Evaluate with multiple metrics\n    metrics = evaluate_with_metrics(model, test_loader, device, target_indices)\n\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Test Set Evaluation (Best Model)\")\n    print(\"=\" * 50)\n    print(f\"MSE:  {metrics['mse']:.6f}\")\n    print(f\"RMSE: {metrics['rmse']:.6f}\")\n    print(f\"MAE:  {metrics['mae']:.6f}\")\n    print(f\"R\u00b2:   {metrics['r2']:.6f}\")\n    print(\"=\" * 50 + \"\\n\")\n</code></pre>"},{"location":"my_api/#model","title":"model","text":""},{"location":"my_api/#project_name.model","title":"project_name.model","text":""},{"location":"my_api/#project_name.model.GraphNeuralNetwork","title":"GraphNeuralNetwork","text":"<p>               Bases: <code>Module</code></p> <p>Graph Neural Network for molecular property regression.</p> Source code in <code>src/project_name/model.py</code> <pre><code>class GraphNeuralNetwork(nn.Module):\n    \"\"\"Graph Neural Network for molecular property regression.\"\"\"\n\n    def __init__(\n        self,\n        num_node_features: int = 11,\n        num_edge_features: int = 4,\n        hidden_dim: int = 128,\n        num_layers: int = 3,\n        output_dim: int = 1,\n        dropout: float = 0.1,\n    ) -&gt; None:\n        \"\"\"Initialize the GNN model.\n\n        Args:\n            num_node_features: Number of node (atom) features.\n            num_edge_features: Number of edge (bond) features.\n            hidden_dim: Number of hidden channels.\n            num_layers: Number of GraphConv layers.\n\n\n            output_dim: Output dimension (1 for single property regression).\n            dropout: Dropout rate for regularization.\n        \"\"\"\n        super().__init__()\n        self.dropout_rate = dropout\n\n        self.initial_embedding = nn.Linear(num_node_features, hidden_dim)\n\n        self.conv_layers = nn.ModuleList([GraphConv(hidden_dim, hidden_dim) for _ in range(num_layers)])\n\n        self.pool = global_mean_pool\n\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, output_dim),\n        )\n\n    def forward(self, data) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the model.\n\n        Args:\n            data: PyTorch Geometric Data object with x, edge_index, and batch attributes.\n\n        Returns:\n            Predicted property values.\n        \"\"\"\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n\n        x = self.initial_embedding(x)\n        x = F.relu(x)\n\n        for conv in self.conv_layers:\n            x = conv(x, edge_index)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n\n        x = self.pool(x, batch)\n\n        x = self.mlp(x)\n\n        return x\n</code></pre>"},{"location":"my_api/#project_name.model.GraphNeuralNetwork.__init__","title":"__init__","text":"<pre><code>__init__(num_node_features: int = 11, num_edge_features: int = 4, hidden_dim: int = 128, num_layers: int = 3, output_dim: int = 1, dropout: float = 0.1) -&gt; None\n</code></pre> <p>Initialize the GNN model.</p> <p>Parameters:</p> Name Type Description Default <code>num_node_features</code> <code>int</code> <p>Number of node (atom) features.</p> <code>11</code> <code>num_edge_features</code> <code>int</code> <p>Number of edge (bond) features.</p> <code>4</code> <code>hidden_dim</code> <code>int</code> <p>Number of hidden channels.</p> <code>128</code> <code>num_layers</code> <code>int</code> <p>Number of GraphConv layers.</p> <code>3</code> <code>output_dim</code> <code>int</code> <p>Output dimension (1 for single property regression).</p> <code>1</code> <code>dropout</code> <code>float</code> <p>Dropout rate for regularization.</p> <code>0.1</code> Source code in <code>src/project_name/model.py</code> <pre><code>def __init__(\n    self,\n    num_node_features: int = 11,\n    num_edge_features: int = 4,\n    hidden_dim: int = 128,\n    num_layers: int = 3,\n    output_dim: int = 1,\n    dropout: float = 0.1,\n) -&gt; None:\n    \"\"\"Initialize the GNN model.\n\n    Args:\n        num_node_features: Number of node (atom) features.\n        num_edge_features: Number of edge (bond) features.\n        hidden_dim: Number of hidden channels.\n        num_layers: Number of GraphConv layers.\n\n\n        output_dim: Output dimension (1 for single property regression).\n        dropout: Dropout rate for regularization.\n    \"\"\"\n    super().__init__()\n    self.dropout_rate = dropout\n\n    self.initial_embedding = nn.Linear(num_node_features, hidden_dim)\n\n    self.conv_layers = nn.ModuleList([GraphConv(hidden_dim, hidden_dim) for _ in range(num_layers)])\n\n    self.pool = global_mean_pool\n\n    self.mlp = nn.Sequential(\n        nn.Linear(hidden_dim, hidden_dim),\n        nn.ReLU(),\n        nn.Dropout(dropout),\n        nn.Linear(hidden_dim, hidden_dim // 2),\n        nn.ReLU(),\n        nn.Dropout(dropout),\n        nn.Linear(hidden_dim // 2, output_dim),\n    )\n</code></pre>"},{"location":"my_api/#project_name.model.GraphNeuralNetwork.forward","title":"forward","text":"<pre><code>forward(data) -&gt; torch.Tensor\n</code></pre> <p>Forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>PyTorch Geometric Data object with x, edge_index, and batch attributes.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Predicted property values.</p> Source code in <code>src/project_name/model.py</code> <pre><code>def forward(self, data) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the model.\n\n    Args:\n        data: PyTorch Geometric Data object with x, edge_index, and batch attributes.\n\n    Returns:\n        Predicted property values.\n    \"\"\"\n    x, edge_index, batch = data.x, data.edge_index, data.batch\n\n    x = self.initial_embedding(x)\n    x = F.relu(x)\n\n    for conv in self.conv_layers:\n        x = conv(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n\n    x = self.pool(x, batch)\n\n    x = self.mlp(x)\n\n    return x\n</code></pre>"},{"location":"my_api/#profiling","title":"profiling","text":""},{"location":"my_api/#project_name.profiling","title":"project_name.profiling","text":"<p>Profiling utilities for training and evaluation.</p>"},{"location":"my_api/#project_name.profiling.TrainingProfiler","title":"TrainingProfiler","text":"<p>Manages profiling across entire training session.</p> Source code in <code>src/project_name/profiling.py</code> <pre><code>class TrainingProfiler:\n    \"\"\"Manages profiling across entire training session.\"\"\"\n\n    def __init__(\n        self,\n        enabled: bool = False,\n        output_dir: Optional[Path] = None,\n        warmup_steps: int = 1,\n        active_steps: int = 10,\n        repeat_steps: int = 1,\n    ) -&gt; None:\n        \"\"\"Initialize the training profiler.\n\n        Args:\n            enabled: Whether to enable profiling.\n            output_dir: Directory to save profiling results.\n        \"\"\"\n        self.enabled = enabled\n        self.output_dir = output_dir or Path(\"profiling_results/run\")\n        self.prof: Optional[profile] = None\n\n        if self.enabled:\n            self.output_dir.mkdir(parents=True, exist_ok=True)\n            self.prof = profile(\n                activities=[ProfilerActivity.CPU]\n                if not torch.cuda.is_available()\n                else [ProfilerActivity.CPU, ProfilerActivity.CUDA],\n                record_shapes=True,\n                profile_memory=True,\n                schedule=torch.profiler.schedule(\n                    wait=0,\n                    warmup=warmup_steps,\n                    active=active_steps,\n                    repeat=repeat_steps,\n                ),\n                on_trace_ready=tensorboard_trace_handler(output_dir),\n            )\n            self.prof.__enter__()\n\n    def step(self) -&gt; None:\n        \"\"\"Record a step (epoch) in the profiler.\"\"\"\n        if self.prof:\n            self.prof.step()\n\n    def finalize(self) -&gt; None:\n        \"\"\"Finalize profiling and export trace.\"\"\"\n        if self.prof:\n            self.prof.__exit__(None, None, None)\n            print(f\"\u2705 Profiling trace saved to {self.output_dir}\")\n</code></pre>"},{"location":"my_api/#project_name.profiling.TrainingProfiler.__init__","title":"__init__","text":"<pre><code>__init__(enabled: bool = False, output_dir: Optional[Path] = None, warmup_steps: int = 1, active_steps: int = 10, repeat_steps: int = 1) -&gt; None\n</code></pre> <p>Initialize the training profiler.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>Whether to enable profiling.</p> <code>False</code> <code>output_dir</code> <code>Optional[Path]</code> <p>Directory to save profiling results.</p> <code>None</code> Source code in <code>src/project_name/profiling.py</code> <pre><code>def __init__(\n    self,\n    enabled: bool = False,\n    output_dir: Optional[Path] = None,\n    warmup_steps: int = 1,\n    active_steps: int = 10,\n    repeat_steps: int = 1,\n) -&gt; None:\n    \"\"\"Initialize the training profiler.\n\n    Args:\n        enabled: Whether to enable profiling.\n        output_dir: Directory to save profiling results.\n    \"\"\"\n    self.enabled = enabled\n    self.output_dir = output_dir or Path(\"profiling_results/run\")\n    self.prof: Optional[profile] = None\n\n    if self.enabled:\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        self.prof = profile(\n            activities=[ProfilerActivity.CPU]\n            if not torch.cuda.is_available()\n            else [ProfilerActivity.CPU, ProfilerActivity.CUDA],\n            record_shapes=True,\n            profile_memory=True,\n            schedule=torch.profiler.schedule(\n                wait=0,\n                warmup=warmup_steps,\n                active=active_steps,\n                repeat=repeat_steps,\n            ),\n            on_trace_ready=tensorboard_trace_handler(output_dir),\n        )\n        self.prof.__enter__()\n</code></pre>"},{"location":"my_api/#project_name.profiling.TrainingProfiler.finalize","title":"finalize","text":"<pre><code>finalize() -&gt; None\n</code></pre> <p>Finalize profiling and export trace.</p> Source code in <code>src/project_name/profiling.py</code> <pre><code>def finalize(self) -&gt; None:\n    \"\"\"Finalize profiling and export trace.\"\"\"\n    if self.prof:\n        self.prof.__exit__(None, None, None)\n        print(f\"\u2705 Profiling trace saved to {self.output_dir}\")\n</code></pre>"},{"location":"my_api/#project_name.profiling.TrainingProfiler.step","title":"step","text":"<pre><code>step() -&gt; None\n</code></pre> <p>Record a step (epoch) in the profiler.</p> Source code in <code>src/project_name/profiling.py</code> <pre><code>def step(self) -&gt; None:\n    \"\"\"Record a step (epoch) in the profiler.\"\"\"\n    if self.prof:\n        self.prof.step()\n</code></pre>"},{"location":"my_api/#project_name.profiling.timing_checkpoint","title":"timing_checkpoint","text":"<pre><code>timing_checkpoint(name: str, enabled: bool = True) -&gt; Generator\n</code></pre> <p>Context manager for simple timing measurements.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name for this checkpoint.</p> required <code>enabled</code> <code>bool</code> <p>Whether to enable timing.</p> <code>True</code> <p>Yields:</p> Type Description <code>Generator</code> <p>Dictionary with timing results.</p> Source code in <code>src/project_name/profiling.py</code> <pre><code>@contextmanager\ndef timing_checkpoint(name: str, enabled: bool = True) -&gt; Generator:\n    \"\"\"Context manager for simple timing measurements.\n\n    Args:\n        name: Name for this checkpoint.\n        enabled: Whether to enable timing.\n\n    Yields:\n        Dictionary with timing results.\n    \"\"\"\n    result = {\"name\": name, \"duration\": 0.0}\n\n    if not enabled:\n        yield result\n        return\n\n    start = time.perf_counter()\n    try:\n        yield result\n    finally:\n        result[\"duration\"] = time.perf_counter() - start\n        print(f\"\u23f1\ufe0f  {name}: {result['duration']:.4f}s\")\n</code></pre>"},{"location":"my_api/#prune","title":"prune","text":""},{"location":"my_api/#project_name.prune","title":"project_name.prune","text":""},{"location":"my_api/#project_name.prune.apply_unstructured_pruning","title":"apply_unstructured_pruning","text":"<pre><code>apply_unstructured_pruning(model: torch.nn.Module, amount: float) -&gt; dict[str, Any]\n</code></pre> <p>Apply unstructured L1 pruning to FC layers only, then make it permanent.</p> Source code in <code>src/project_name/prune.py</code> <pre><code>def apply_unstructured_pruning(\n    model: torch.nn.Module,\n    amount: float,\n) -&gt; dict[str, Any]:\n    \"\"\"Apply unstructured L1 pruning to FC layers only, then make it permanent.\"\"\"\n    if not (0.0 &lt;= amount &lt; 1.0):\n        raise ValueError(f\"Prune amount must be in [0, 1). Got: {amount}\")\n\n    pruned_modules = list(_iter_prunable_weight_params(model))\n    if not pruned_modules:\n        logger.warning(\"No prunable fully-connected (nn.Linear) layers found.\")\n        return {\"modules_pruned\": 0, \"global_sparsity\": 0.0}\n\n    for (m, pname) in pruned_modules:\n        prune.l1_unstructured(m, name=pname, amount=amount)\n\n    for (m, pname) in pruned_modules:\n        prune.remove(m, pname)\n\n    total_elems = 0\n    zero_elems = 0\n    for (m, pname) in pruned_modules:\n        w = getattr(m, pname)\n        total_elems += w.numel()\n        zero_elems += int((w == 0).sum().item())\n\n    global_sparsity = (zero_elems / total_elems) if total_elems &gt; 0 else 0.0\n    return {\n        \"modules_pruned\": len(pruned_modules),\n        \"global_sparsity\": global_sparsity,\n        \"zero_elems\": zero_elems,\n        \"total_elems\": total_elems,\n    }\n</code></pre>"},{"location":"my_api/#project_name.prune.evaluate_mse","title":"evaluate_mse","text":"<pre><code>evaluate_mse(model: torch.nn.Module, loader: DataLoader, device: torch.device, target_indices: Sequence[int]) -&gt; float\n</code></pre> <p>Mean MSE per graph (matches your train/eval convention).</p> Source code in <code>src/project_name/prune.py</code> <pre><code>@torch.no_grad()\ndef evaluate_mse(\n    model: torch.nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n    target_indices: Sequence[int],\n) -&gt; float:\n    \"\"\"Mean MSE per graph (matches your train/eval convention).\"\"\"\n    model.eval()\n\n    total_loss: float = 0.0\n    num_samples: int = 0\n    target_idx = list(target_indices)\n\n    for batch in loader:\n        batch = batch.to(device)\n        pred = model(batch)\n        target = batch.y[:, target_idx]\n        loss = torch.nn.functional.mse_loss(pred, target)\n        total_loss += float(loss.item()) * batch.num_graphs\n        num_samples += batch.num_graphs\n\n    return total_loss / max(1, num_samples)\n</code></pre>"},{"location":"my_api/#project_name.prune.measure_inference_latency","title":"measure_inference_latency","text":"<pre><code>measure_inference_latency(model: torch.nn.Module, loader: DataLoader, device: torch.device, *, warmup_batches: int = 10, timed_batches: int = 50) -&gt; dict[str, float]\n</code></pre> <p>Measures average latency per batch (ms) over a fixed number of batches.</p> <p>Notes: - Uses torch.inference_mode() via @torch.no_grad() + model.eval() - Syncs CUDA for accurate timing</p> Source code in <code>src/project_name/prune.py</code> <pre><code>@torch.no_grad()\ndef measure_inference_latency(\n    model: torch.nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n    *,\n    warmup_batches: int = 10,\n    timed_batches: int = 50,\n) -&gt; dict[str, float]:\n    \"\"\"\n    Measures average latency per batch (ms) over a fixed number of batches.\n\n    Notes:\n    - Uses torch.inference_mode() via @torch.no_grad() + model.eval()\n    - Syncs CUDA for accurate timing\n    \"\"\"\n    model.eval()\n\n    def _sync() -&gt; None:\n        if device.type == \"cuda\":\n            torch.cuda.synchronize()\n\n    it = iter(loader)\n\n    # Warmup\n    for _ in range(warmup_batches):\n        try:\n            batch = next(it)\n        except StopIteration:\n            it = iter(loader)\n            batch = next(it)\n        batch = batch.to(device)\n        _ = model(batch)\n    _sync()\n\n    # Timed\n    times: list[float] = []\n    for _ in range(timed_batches):\n        try:\n            batch = next(it)\n        except StopIteration:\n            it = iter(loader)\n            batch = next(it)\n        batch = batch.to(device)\n\n        _sync()\n        t0 = time.perf_counter()\n        _ = model(batch)\n        _sync()\n        t1 = time.perf_counter()\n        times.append(t1 - t0)\n\n    if not times:\n        return {\"ms_per_batch\": 0.0, \"batches\": 0}\n\n    avg_s = sum(times) / len(times)\n    return {\"ms_per_batch\": avg_s * 1000.0, \"batches\": float(len(times))}\n</code></pre>"},{"location":"my_api/#quantize","title":"quantize","text":""},{"location":"my_api/#project_name.quantize","title":"project_name.quantize","text":""},{"location":"my_api/#project_name.quantize.measure_inference_latency","title":"measure_inference_latency","text":"<pre><code>measure_inference_latency(model: torch.nn.Module, loader: DataLoader, device: torch.device, *, warmup_batches: int = 10, timed_batches: int = 50) -&gt; dict[str, float]\n</code></pre> <p>Average latency per batch in ms. Note: for quantized CPU models this is the typical use case.</p> Source code in <code>src/project_name/quantize.py</code> <pre><code>@torch.no_grad()\ndef measure_inference_latency(\n    model: torch.nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n    *,\n    warmup_batches: int = 10,\n    timed_batches: int = 50,\n) -&gt; dict[str, float]:\n    \"\"\"\n    Average latency per batch in ms.\n    Note: for quantized CPU models this is the typical use case.\n    \"\"\"\n    model.eval()\n    it = iter(loader)\n\n    # Warmup\n    for _ in range(warmup_batches):\n        try:\n            batch = next(it)\n        except StopIteration:\n            it = iter(loader)\n            batch = next(it)\n        batch = batch.to(device)\n        _ = model(batch)\n\n    times: list[float] = []\n    for _ in range(timed_batches):\n        try:\n            batch = next(it)\n        except StopIteration:\n            it = iter(loader)\n            batch = next(it)\n\n        batch = batch.to(device)\n        t0 = time.perf_counter()\n        _ = model(batch)\n        t1 = time.perf_counter()\n        times.append(t1 - t0)\n\n    if not times:\n        return {\"ms_per_batch\": 0.0, \"batches\": 0.0}\n\n    avg_s = sum(times) / len(times)\n    return {\"ms_per_batch\": avg_s * 1000.0, \"batches\": float(len(times))}\n</code></pre>"},{"location":"my_api/#project_name.quantize.quantize_full_model","title":"quantize_full_model","text":"<pre><code>quantize_full_model(model: torch.nn.Module, scheme: str) -&gt; torch.nn.Module\n</code></pre> <p>Apply weight-only INT8 quantization to all linear layers in the model, including those nested inside GraphConv blocks. Uses torchao when available and falls back to the torch.ao dynamic quantization API otherwise.</p> scheme <ul> <li>\"torchao_int8_weight_only\" (default)</li> <li>\"torch_ao_dynamic\" (fallback-style dynamic quantization)</li> </ul> Source code in <code>src/project_name/quantize.py</code> <pre><code>def quantize_full_model(model: torch.nn.Module, scheme: str) -&gt; torch.nn.Module:\n    \"\"\"\n    Apply weight-only INT8 quantization to *all* linear layers in the model, including those\n    nested inside GraphConv blocks. Uses torchao when available and falls back to the\n    torch.ao dynamic quantization API otherwise.\n\n    scheme:\n      - \"torchao_int8_weight_only\" (default)\n      - \"torch_ao_dynamic\" (fallback-style dynamic quantization)\n    \"\"\"\n    if scheme == \"torch_ao_dynamic\":\n        from torch.ao.quantization import quantize_dynamic  # older weights-only API\n        return quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n\n    # default: torchao\n    try:\n        from torchao.quantization import quantize_\n        from torchao.quantization import Int8WeightOnlyConfig\n    except Exception as e:\n        logger.warning(\"torchao not available (%s). Falling back to torch.ao.quantization.quantize_dynamic.\", e)\n        from torch.ao.quantization import quantize_dynamic\n        return quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n\n    # torchao quantize_ is inplace; returns None or model depending on version\n    quantize_(model, Int8WeightOnlyConfig())  #  [oai_citation:3\u2021PyTorch Documentation](https://docs.pytorch.org/ao/stable/generated/torchao.quantization.quantize_.html)\n    return model\n</code></pre>"},{"location":"my_api/#train","title":"train","text":""},{"location":"my_api/#project_name.train","title":"project_name.train","text":""},{"location":"my_api/#project_name.train.train","title":"train","text":"<pre><code>train(cfg: DictConfig) -&gt; None\n</code></pre> <p>Train the GNN model on QM9 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra configuration object containing all parameters.</p> required Source code in <code>src/project_name/train.py</code> <pre><code>@hydra.main(version_base=None, config_path=_CONFIG_PATH, config_name=\"config\")\ndef train(cfg: DictConfig) -&gt; None:\n    \"\"\"Train the GNN model on QM9 dataset.\n\n    Args:\n        cfg: Hydra configuration object containing all parameters.\n    \"\"\"\n    device: torch.device = _get_device()\n    logger.info(\"Using device: %s\", device)\n\n    model_dir: Path = get_data_path(\n        cfg.training.model_dir,\n        gcs_bucket=OmegaConf.select(cfg, \"training.gcs_bucket\"),\n    )\n    model_dir.mkdir(parents=True, exist_ok=True)\n    print(cfg)\n\n    profile: bool = cfg.training.profile\n    profiler_run_dir: str = cfg.training.profiler_run_dir\n    run = _init_wandb(cfg)\n    if run is not None:\n        logger.info(\"wandb logging enabled (run: %s)\", run.id)\n    else:\n        logger.info(\"wandb logging disabled\")\n    with timing_checkpoint(\"Load dataset\", enabled=profile):\n        logger.info(\"Loading QM9 dataset...\")\n        data_path = get_data_path(\n            cfg.training.data_path,\n            gcs_bucket=OmegaConf.select(cfg, \"training.gcs_bucket\"),\n        )\n        dataset: Dataset = QM9Dataset(data_path)\n\n    # Apply normalization transform\n    dataset.transform = NormalizeScale()\n\n    # Split dataset\n    n: int = len(dataset)\n    train_size: int = int(cfg.training.train_ratio * n)\n    val_size: int = int(cfg.training.val_ratio * n)\n    test_size: int = n - train_size - val_size\n\n    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n        dataset,\n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(cfg.seed),\n    )\n\n    # Create data loaders\n    # Create data loaders (parallel loading)\n    workers = _num_workers(cfg)\n    logger.info(\"DataLoader num_workers=%d\", workers)\n\n    train_loader: DataLoader = DataLoader(\n        train_dataset,\n        batch_size=cfg.training.batch_size,\n        shuffle=True,\n        num_workers=workers,\n        pin_memory=torch.cuda.is_available(),\n        persistent_workers=(workers &gt; 0),\n    )\n\n    val_loader: DataLoader = DataLoader(\n        val_dataset,\n        batch_size=cfg.training.batch_size,\n        shuffle=False,\n        num_workers=workers,\n        pin_memory=torch.cuda.is_available(),\n        persistent_workers=(workers &gt; 0),\n    )\n\n    test_loader: DataLoader = DataLoader(\n        test_dataset,\n        batch_size=cfg.training.batch_size,\n        shuffle=False,\n        num_workers=workers,\n        pin_memory=torch.cuda.is_available(),\n        persistent_workers=(workers &gt; 0),\n    )\n\n    logger.info(\"Dataset split - Train: %d, Val: %d, Test: %d\", len(train_dataset), len(val_dataset), len(test_dataset))\n\n    # Get target indices and infer output dimension\n    target_indices: list[int] = list(cfg.training.target_indices)\n    num_targets: int = len(target_indices)\n    logger.info(\"Predicting %d target(s): %s\", num_targets, target_indices)\n\n    # Initialize model\n    model: GraphNeuralNetwork | nn.DataParallel = GraphNeuralNetwork(\n        num_node_features=cfg.model.num_node_features,\n        hidden_dim=cfg.model.hidden_dim,\n        num_layers=cfg.model.num_layers,\n        output_dim=num_targets,\n    ).to(device)\n\n    if torch.cuda.is_available() and torch.cuda.device_count() &gt; 1:\n        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n\n    optimizer: Optimizer = torch.optim.Adam(model.parameters(), lr=cfg.training.learning_rate)\n\n    # Early stopping variables\n    best_val_loss: float = float(\"inf\")\n    patience: int = cfg.training.patience\n    patience_counter: int = 0\n\n    logger.info(\n        \"Starting training for %d epochs (batch_size=%d, lr=%g, patience=%d)\",\n        cfg.training.epochs,\n        cfg.training.batch_size,\n        cfg.training.learning_rate,\n        patience,\n    )\n    profiler = TrainingProfiler(enabled=profile, output_dir=Path(f\"profiling_results/{profiler_run_dir}\"))\n\n    for epoch in range(1, cfg.training.epochs + 1):\n        train_loss: float = train_epoch(model, train_loader, optimizer, device, target_indices)\n\n        # compute validation metrics (mse/rmse/mae/r2)\n        val_metrics = evaluate_with_metrics(model, val_loader, device, target_indices)\n        val_loss: float = float(val_metrics[\"mse\"])  # keep early-stopping tied to MSE\n\n        if epoch % LOG_INTERVAL == 0 or epoch == 1:\n            logger.info(\n                \"Epoch %3d | Train Loss: %.6f | Val MSE: %.6f | Val RMSE: %.6f | Val MAE: %.6f | Val R2: %.6f\",\n                epoch, train_loss, val_metrics[\"mse\"], val_metrics[\"rmse\"], val_metrics[\"mae\"], val_metrics[\"r2\"]\n            )\n\n        improved = val_loss &lt; best_val_loss\n        if improved:\n            best_val_loss = val_loss\n            patience_counter = 0\n            best_model_path: Path = model_dir / \"best_model.pt\"\n            torch.save(\n                model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),\n                best_model_path,\n            )\n            logger.debug(\"Saved best model to %s\", best_model_path)\n        else:\n            patience_counter += 1\n\n        if run is not None:\n            wandb.log(\n                {\n                    \"epoch\": epoch,\n                    \"loss/train\": train_loss,\n\n                    # keep your existing val loss key if you want\n                    \"loss/val\": val_loss,\n\n                    # add full validation metrics\n                    \"val/mse\": val_metrics[\"mse\"],\n                    \"val/rmse\": val_metrics[\"rmse\"],\n                    \"val/mae\": val_metrics[\"mae\"],\n                    \"val/r2\": val_metrics[\"r2\"],\n\n                    \"early_stopping/patience_counter\": patience_counter,\n                    \"early_stopping/best_val_loss\": best_val_loss,\n                }\n            )\n\n        if patience_counter &gt;= patience:\n            logger.info(\"Early stopping triggered at epoch %d (best_val_loss=%.6f)\", epoch, best_val_loss)\n            break\n\n        profiler.step()\n    profiler.finalize()\n\n    # Load best model and evaluate on test set\n    best_model_path = model_dir / \"best_model.pt\"\n\n    try:\n        state = torch.load(best_model_path, weights_only=True)\n    except TypeError:\n        state = torch.load(best_model_path)\n\n    if isinstance(model, nn.DataParallel):\n        model.module.load_state_dict(state)\n    else:\n        model.load_state_dict(state)\n\n    test_metrics: dict[str, float] = evaluate_with_metrics(model, test_loader, device, target_indices)\n    test_loss: float = float(test_metrics[\"mse\"])\n    logger.info(\"Final test loss: %.6f\", test_loss)\n\n    # Save final model\n    final_model_path: Path = model_dir / \"final_model.pt\"\n    torch.save(\n    model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),\n    final_model_path,\n    )  \n    logger.info(\"Training complete. Models saved to %s\", model_dir)\n\n    # wandb: final logs\n    if run is not None:\n        # Log full test metrics (assumes you computed `test_metrics` as shown before)\n        wandb.log(\n            {\n                \"loss/test\": test_loss,              # keep compatibility (MSE)\n                \"test/mse\": test_metrics[\"mse\"],\n                \"test/rmse\": test_metrics[\"rmse\"],\n                \"test/mae\": test_metrics[\"mae\"],\n                \"test/r2\": test_metrics[\"r2\"],\n            }\n        )\n\n        # Optionally log model artifact\n        artifact = None\n        if bool(OmegaConf.select(cfg, \"wandb.log_artifacts\", default=True)):\n            artifact = wandb.Artifact(\n                name=\"qm9-gnn\",\n                type=\"model\",\n                description=\"Trained model\",\n                metadata={\n                    \"target_indices\": target_indices,\n                    \"best_val_loss\": best_val_loss,\n                    \"test_mse\": test_metrics[\"mse\"],\n                    \"test_rmse\": test_metrics[\"rmse\"],\n                    \"test_mae\": test_metrics[\"mae\"],\n                    \"test_r2\": test_metrics[\"r2\"],\n                },\n            )\n            artifact.add_file(str(best_model_path))\n            run.log_artifact(artifact)\n\n            # Link only if we actually created an artifact\n            run.link_artifact(\n                artifact=artifact,\n                target_path=\"model-registry/mlops-molecules\",\n                aliases=[\"latest\"],\n            )\n\n        wandb.finish()\n</code></pre>"},{"location":"my_api/#project_name.train.train_epoch","title":"train_epoch","text":"<pre><code>train_epoch(model: GraphNeuralNetwork, loader: DataLoader, optimizer: Optimizer, device: torch.device, target_indices: list[int]) -&gt; float\n</code></pre> <p>Train for one epoch.</p> Source code in <code>src/project_name/train.py</code> <pre><code>def train_epoch(\n    model: GraphNeuralNetwork,\n    loader: DataLoader,\n    optimizer: Optimizer,\n    device: torch.device,\n    target_indices: list[int],\n) -&gt; float:\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n    total_loss: float = 0.0\n    num_samples: int = 0\n\n    for batch in loader:\n        batch = batch.to(device)\n        optimizer.zero_grad(set_to_none=True)\n\n        pred: torch.Tensor = model(batch)\n        target: torch.Tensor = batch.y[:, target_indices]\n\n        loss: torch.Tensor = F.mse_loss(pred, target)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * batch.num_graphs\n        num_samples += batch.num_graphs\n\n    return total_loss / num_samples\n</code></pre>"},{"location":"my_api/#utils","title":"utils","text":""},{"location":"my_api/#project_name.utils","title":"project_name.utils","text":"<p>Utility functions for data loading and environment detection.</p>"},{"location":"my_api/#project_name.utils.get_data_path","title":"get_data_path","text":"<pre><code>get_data_path(config_path: str | Path, gcs_bucket: str | None = None) -&gt; Path\n</code></pre> <p>Get the appropriate data path based on the environment.</p> <p>In cloud environments (GCP/Vertex AI), data is mounted to /gcs/. In local environments, data is loaded from the configured path. <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str | Path</code> <p>The data path from config (e.g., 'data' or 'data/processed').</p> required <code>gcs_bucket</code> <code>str | None</code> <p>Optional GCS bucket name. If provided and running in cloud,         will use /gcs/ as the base path. <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path object pointing to the correct data location.</p> Source code in <code>src/project_name/utils.py</code> <pre><code>def get_data_path(config_path: str | Path, gcs_bucket: str | None = None) -&gt; Path:\n    \"\"\"Get the appropriate data path based on the environment.\n\n    In cloud environments (GCP/Vertex AI), data is mounted to /gcs/&lt;bucket-name&gt;.\n    In local environments, data is loaded from the configured path.\n\n    Args:\n        config_path: The data path from config (e.g., 'data' or 'data/processed').\n        gcs_bucket: Optional GCS bucket name. If provided and running in cloud,\n                    will use /gcs/&lt;bucket-name&gt; as the base path.\n\n    Returns:\n        Path object pointing to the correct data location.\n    \"\"\"\n    gcs_mount = Path(\"/gcs\")\n\n    if gcs_mount.exists() and gcs_bucket:\n        data_path = gcs_mount / gcs_bucket / config_path\n    else:\n        data_path = Path(config_path)\n\n    return data_path\n</code></pre>"},{"location":"my_api/#visualize","title":"visualize","text":""},{"location":"my_api/#project_name.visualize","title":"project_name.visualize","text":""},{"location":"training/","title":"Training Pipeline","text":"<p>Training, evaluation, and model optimization workflows.</p>"},{"location":"training/#training-entrypoint","title":"Training entrypoint","text":""},{"location":"training/#project_name.train","title":"project_name.train","text":""},{"location":"training/#project_name.train.train","title":"train","text":"<pre><code>train(cfg: DictConfig) -&gt; None\n</code></pre> <p>Train the GNN model on QM9 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra configuration object containing all parameters.</p> required Source code in <code>src/project_name/train.py</code> <pre><code>@hydra.main(version_base=None, config_path=_CONFIG_PATH, config_name=\"config\")\ndef train(cfg: DictConfig) -&gt; None:\n    \"\"\"Train the GNN model on QM9 dataset.\n\n    Args:\n        cfg: Hydra configuration object containing all parameters.\n    \"\"\"\n    device: torch.device = _get_device()\n    logger.info(\"Using device: %s\", device)\n\n    model_dir: Path = get_data_path(\n        cfg.training.model_dir,\n        gcs_bucket=OmegaConf.select(cfg, \"training.gcs_bucket\"),\n    )\n    model_dir.mkdir(parents=True, exist_ok=True)\n    print(cfg)\n\n    profile: bool = cfg.training.profile\n    profiler_run_dir: str = cfg.training.profiler_run_dir\n    run = _init_wandb(cfg)\n    if run is not None:\n        logger.info(\"wandb logging enabled (run: %s)\", run.id)\n    else:\n        logger.info(\"wandb logging disabled\")\n    with timing_checkpoint(\"Load dataset\", enabled=profile):\n        logger.info(\"Loading QM9 dataset...\")\n        data_path = get_data_path(\n            cfg.training.data_path,\n            gcs_bucket=OmegaConf.select(cfg, \"training.gcs_bucket\"),\n        )\n        dataset: Dataset = QM9Dataset(data_path)\n\n    # Apply normalization transform\n    dataset.transform = NormalizeScale()\n\n    # Split dataset\n    n: int = len(dataset)\n    train_size: int = int(cfg.training.train_ratio * n)\n    val_size: int = int(cfg.training.val_ratio * n)\n    test_size: int = n - train_size - val_size\n\n    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n        dataset,\n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(cfg.seed),\n    )\n\n    # Create data loaders\n    # Create data loaders (parallel loading)\n    workers = _num_workers(cfg)\n    logger.info(\"DataLoader num_workers=%d\", workers)\n\n    train_loader: DataLoader = DataLoader(\n        train_dataset,\n        batch_size=cfg.training.batch_size,\n        shuffle=True,\n        num_workers=workers,\n        pin_memory=torch.cuda.is_available(),\n        persistent_workers=(workers &gt; 0),\n    )\n\n    val_loader: DataLoader = DataLoader(\n        val_dataset,\n        batch_size=cfg.training.batch_size,\n        shuffle=False,\n        num_workers=workers,\n        pin_memory=torch.cuda.is_available(),\n        persistent_workers=(workers &gt; 0),\n    )\n\n    test_loader: DataLoader = DataLoader(\n        test_dataset,\n        batch_size=cfg.training.batch_size,\n        shuffle=False,\n        num_workers=workers,\n        pin_memory=torch.cuda.is_available(),\n        persistent_workers=(workers &gt; 0),\n    )\n\n    logger.info(\"Dataset split - Train: %d, Val: %d, Test: %d\", len(train_dataset), len(val_dataset), len(test_dataset))\n\n    # Get target indices and infer output dimension\n    target_indices: list[int] = list(cfg.training.target_indices)\n    num_targets: int = len(target_indices)\n    logger.info(\"Predicting %d target(s): %s\", num_targets, target_indices)\n\n    # Initialize model\n    model: GraphNeuralNetwork | nn.DataParallel = GraphNeuralNetwork(\n        num_node_features=cfg.model.num_node_features,\n        hidden_dim=cfg.model.hidden_dim,\n        num_layers=cfg.model.num_layers,\n        output_dim=num_targets,\n    ).to(device)\n\n    if torch.cuda.is_available() and torch.cuda.device_count() &gt; 1:\n        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n\n    optimizer: Optimizer = torch.optim.Adam(model.parameters(), lr=cfg.training.learning_rate)\n\n    # Early stopping variables\n    best_val_loss: float = float(\"inf\")\n    patience: int = cfg.training.patience\n    patience_counter: int = 0\n\n    logger.info(\n        \"Starting training for %d epochs (batch_size=%d, lr=%g, patience=%d)\",\n        cfg.training.epochs,\n        cfg.training.batch_size,\n        cfg.training.learning_rate,\n        patience,\n    )\n    profiler = TrainingProfiler(enabled=profile, output_dir=Path(f\"profiling_results/{profiler_run_dir}\"))\n\n    for epoch in range(1, cfg.training.epochs + 1):\n        train_loss: float = train_epoch(model, train_loader, optimizer, device, target_indices)\n\n        # compute validation metrics (mse/rmse/mae/r2)\n        val_metrics = evaluate_with_metrics(model, val_loader, device, target_indices)\n        val_loss: float = float(val_metrics[\"mse\"])  # keep early-stopping tied to MSE\n\n        if epoch % LOG_INTERVAL == 0 or epoch == 1:\n            logger.info(\n                \"Epoch %3d | Train Loss: %.6f | Val MSE: %.6f | Val RMSE: %.6f | Val MAE: %.6f | Val R2: %.6f\",\n                epoch, train_loss, val_metrics[\"mse\"], val_metrics[\"rmse\"], val_metrics[\"mae\"], val_metrics[\"r2\"]\n            )\n\n        improved = val_loss &lt; best_val_loss\n        if improved:\n            best_val_loss = val_loss\n            patience_counter = 0\n            best_model_path: Path = model_dir / \"best_model.pt\"\n            torch.save(\n                model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),\n                best_model_path,\n            )\n            logger.debug(\"Saved best model to %s\", best_model_path)\n        else:\n            patience_counter += 1\n\n        if run is not None:\n            wandb.log(\n                {\n                    \"epoch\": epoch,\n                    \"loss/train\": train_loss,\n\n                    # keep your existing val loss key if you want\n                    \"loss/val\": val_loss,\n\n                    # add full validation metrics\n                    \"val/mse\": val_metrics[\"mse\"],\n                    \"val/rmse\": val_metrics[\"rmse\"],\n                    \"val/mae\": val_metrics[\"mae\"],\n                    \"val/r2\": val_metrics[\"r2\"],\n\n                    \"early_stopping/patience_counter\": patience_counter,\n                    \"early_stopping/best_val_loss\": best_val_loss,\n                }\n            )\n\n        if patience_counter &gt;= patience:\n            logger.info(\"Early stopping triggered at epoch %d (best_val_loss=%.6f)\", epoch, best_val_loss)\n            break\n\n        profiler.step()\n    profiler.finalize()\n\n    # Load best model and evaluate on test set\n    best_model_path = model_dir / \"best_model.pt\"\n\n    try:\n        state = torch.load(best_model_path, weights_only=True)\n    except TypeError:\n        state = torch.load(best_model_path)\n\n    if isinstance(model, nn.DataParallel):\n        model.module.load_state_dict(state)\n    else:\n        model.load_state_dict(state)\n\n    test_metrics: dict[str, float] = evaluate_with_metrics(model, test_loader, device, target_indices)\n    test_loss: float = float(test_metrics[\"mse\"])\n    logger.info(\"Final test loss: %.6f\", test_loss)\n\n    # Save final model\n    final_model_path: Path = model_dir / \"final_model.pt\"\n    torch.save(\n    model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),\n    final_model_path,\n    )  \n    logger.info(\"Training complete. Models saved to %s\", model_dir)\n\n    # wandb: final logs\n    if run is not None:\n        # Log full test metrics (assumes you computed `test_metrics` as shown before)\n        wandb.log(\n            {\n                \"loss/test\": test_loss,              # keep compatibility (MSE)\n                \"test/mse\": test_metrics[\"mse\"],\n                \"test/rmse\": test_metrics[\"rmse\"],\n                \"test/mae\": test_metrics[\"mae\"],\n                \"test/r2\": test_metrics[\"r2\"],\n            }\n        )\n\n        # Optionally log model artifact\n        artifact = None\n        if bool(OmegaConf.select(cfg, \"wandb.log_artifacts\", default=True)):\n            artifact = wandb.Artifact(\n                name=\"qm9-gnn\",\n                type=\"model\",\n                description=\"Trained model\",\n                metadata={\n                    \"target_indices\": target_indices,\n                    \"best_val_loss\": best_val_loss,\n                    \"test_mse\": test_metrics[\"mse\"],\n                    \"test_rmse\": test_metrics[\"rmse\"],\n                    \"test_mae\": test_metrics[\"mae\"],\n                    \"test_r2\": test_metrics[\"r2\"],\n                },\n            )\n            artifact.add_file(str(best_model_path))\n            run.log_artifact(artifact)\n\n            # Link only if we actually created an artifact\n            run.link_artifact(\n                artifact=artifact,\n                target_path=\"model-registry/mlops-molecules\",\n                aliases=[\"latest\"],\n            )\n\n        wandb.finish()\n</code></pre>"},{"location":"training/#project_name.train.train_epoch","title":"train_epoch","text":"<pre><code>train_epoch(model: GraphNeuralNetwork, loader: DataLoader, optimizer: Optimizer, device: torch.device, target_indices: list[int]) -&gt; float\n</code></pre> <p>Train for one epoch.</p> Source code in <code>src/project_name/train.py</code> <pre><code>def train_epoch(\n    model: GraphNeuralNetwork,\n    loader: DataLoader,\n    optimizer: Optimizer,\n    device: torch.device,\n    target_indices: list[int],\n) -&gt; float:\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n    total_loss: float = 0.0\n    num_samples: int = 0\n\n    for batch in loader:\n        batch = batch.to(device)\n        optimizer.zero_grad(set_to_none=True)\n\n        pred: torch.Tensor = model(batch)\n        target: torch.Tensor = batch.y[:, target_indices]\n\n        loss: torch.Tensor = F.mse_loss(pred, target)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * batch.num_graphs\n        num_samples += batch.num_graphs\n\n    return total_loss / num_samples\n</code></pre>"},{"location":"training/#evaluation","title":"Evaluation","text":""},{"location":"training/#project_name.evaluate","title":"project_name.evaluate","text":""},{"location":"training/#project_name.evaluate.evaluate","title":"evaluate","text":"<pre><code>evaluate(model: GraphNeuralNetwork, loader: DataLoader, device: torch.device, target_indices: Sequence[int]) -&gt; float\n</code></pre> <p>Evaluate model on a dataloader.</p> <p>Computes mean MSE loss per graph over the entire loader, matching train_epoch.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GraphNeuralNetwork</code> <p>Trained GNN model.</p> required <code>loader</code> <code>DataLoader</code> <p>DataLoader for validation/test set.</p> required <code>device</code> <code>device</code> <p>Torch device.</p> required <code>target_indices</code> <code>Sequence[int]</code> <p>Indices of target properties in batch.y.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean MSE loss per graph.</p> Source code in <code>src/project_name/evaluate.py</code> <pre><code>@torch.no_grad()\ndef evaluate(\n    model: GraphNeuralNetwork,\n    loader: DataLoader,\n    device: torch.device,\n    target_indices: Sequence[int],\n) -&gt; float:\n    \"\"\"Evaluate model on a dataloader.\n\n    Computes mean MSE loss per graph over the entire loader, matching train_epoch.\n\n    Args:\n        model: Trained GNN model.\n        loader: DataLoader for validation/test set.\n        device: Torch device.\n        target_indices: Indices of target properties in batch.y.\n\n    Returns:\n        Mean MSE loss per graph.\n    \"\"\"\n    model.eval()\n\n    total_loss: float = 0.0\n    num_samples: int = 0\n\n    target_idx = list(target_indices)\n\n    for batch in loader:\n        batch = batch.to(device)\n\n        pred: torch.Tensor = model(batch)\n        target: torch.Tensor = batch.y[:, target_idx]\n\n        loss: torch.Tensor = F.mse_loss(pred, target)\n        total_loss += loss.item() * batch.num_graphs\n        num_samples += batch.num_graphs\n\n    if num_samples == 0:\n        return 0.0\n\n    return total_loss / num_samples\n</code></pre>"},{"location":"training/#project_name.evaluate.evaluate_with_metrics","title":"evaluate_with_metrics","text":"<pre><code>evaluate_with_metrics(model: GraphNeuralNetwork, loader: DataLoader, device: torch.device, target_indices: Sequence[int]) -&gt; dict[str, float]\n</code></pre> <p>Evaluate model on a dataloader with multiple metrics.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GraphNeuralNetwork</code> <p>Trained GNN model.</p> required <code>loader</code> <code>DataLoader</code> <p>DataLoader for validation/test set.</p> required <code>device</code> <code>device</code> <p>Torch device.</p> required <code>target_indices</code> <code>Sequence[int]</code> <p>Indices of target properties in batch.y.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with metrics: mse, rmse, mae, r2.</p> Source code in <code>src/project_name/evaluate.py</code> <pre><code>@torch.no_grad()\ndef evaluate_with_metrics(\n    model: GraphNeuralNetwork,\n    loader: DataLoader,\n    device: torch.device,\n    target_indices: Sequence[int],\n) -&gt; dict[str, float]:\n    \"\"\"Evaluate model on a dataloader with multiple metrics.\n\n    Args:\n        model: Trained GNN model.\n        loader: DataLoader for validation/test set.\n        device: Torch device.\n        target_indices: Indices of target properties in batch.y.\n\n    Returns:\n        Dictionary with metrics: mse, rmse, mae, r2.\n    \"\"\"\n    model.eval()\n\n    all_preds: list[torch.Tensor] = []\n    all_targets: list[torch.Tensor] = []\n\n    target_idx = list(target_indices)\n\n    for batch in loader:\n        batch = batch.to(device)\n\n        pred: torch.Tensor = model(batch)\n        target: torch.Tensor = batch.y[:, target_idx]\n\n        all_preds.append(pred)\n        all_targets.append(target)\n\n    if len(all_preds) == 0:\n        return {\"mse\": 0.0, \"rmse\": 0.0, \"mae\": 0.0, \"r2\": 0.0}\n\n    predictions = torch.cat(all_preds, dim=0)\n    targets = torch.cat(all_targets, dim=0)\n\n    # MSE\n    mse = F.mse_loss(predictions, targets).item()\n\n    # RMSE\n    rmse = torch.sqrt(F.mse_loss(predictions, targets)).item()\n\n    # MAE\n    mae = F.l1_loss(predictions, targets).item()\n\n    # R\u00b2 score\n    ss_res = torch.sum((targets - predictions) ** 2).item()\n    ss_tot = torch.sum((targets - torch.mean(targets)) ** 2).item()\n    r2 = 1 - (ss_res / ss_tot) if ss_tot &gt; 0 else 0.0\n\n    return {\n        \"mse\": mse,\n        \"rmse\": rmse,\n        \"mae\": mae,\n        \"r2\": r2,\n    }\n</code></pre>"},{"location":"training/#project_name.evaluate.get_device","title":"get_device","text":"<pre><code>get_device() -&gt; torch.device\n</code></pre> <p>Get the best available device for computation.</p> Source code in <code>src/project_name/evaluate.py</code> <pre><code>def get_device() -&gt; torch.device:\n    \"\"\"Get the best available device for computation.\"\"\"\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    if torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    return torch.device(\"cpu\")\n</code></pre>"},{"location":"training/#project_name.evaluate.main","title":"main","text":"<pre><code>main(cfg: DictConfig) -&gt; None\n</code></pre> <p>Load best model and evaluate on test set with comprehensive metrics.</p> Source code in <code>src/project_name/evaluate.py</code> <pre><code>@hydra.main(version_base=None, config_path=\"../../configs\", config_name=\"config\")\ndef main(cfg: DictConfig) -&gt; None:\n    \"\"\"Load best model and evaluate on test set with comprehensive metrics.\"\"\"\n    device = get_device()\n\n    # Load dataset\n    dataset = QM9Dataset(cfg.training.data_path)\n    dataset.transform = NormalizeScale()\n\n    n = len(dataset)\n    train_size = int(cfg.training.train_ratio * n)\n    val_size = int(cfg.training.val_ratio * n)\n    test_size = n - train_size - val_size\n\n    _, _, test_dataset = torch.utils.data.random_split(\n        dataset,\n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(cfg.seed),\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=cfg.training.batch_size,\n        shuffle=False,\n    )\n\n    target_indices = list(cfg.training.target_indices)\n    num_targets = len(target_indices)\n\n    # Build model\n    model = GraphNeuralNetwork(\n        num_node_features=cfg.model.num_node_features,\n        hidden_dim=cfg.model.hidden_dim,\n        num_layers=cfg.model.num_layers,\n        output_dim=num_targets,\n    ).to(device)\n\n    # Load best model\n    best_model_path = Path(cfg.training.model_dir) / \"best_model.pt\"\n    print(f\"Loading model from: {best_model_path}\")\n\n    try:\n        state = torch.load(best_model_path, weights_only=True)\n    except TypeError:\n        state = torch.load(best_model_path)\n\n    model.load_state_dict(state)\n\n    # Evaluate with multiple metrics\n    metrics = evaluate_with_metrics(model, test_loader, device, target_indices)\n\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Test Set Evaluation (Best Model)\")\n    print(\"=\" * 50)\n    print(f\"MSE:  {metrics['mse']:.6f}\")\n    print(f\"RMSE: {metrics['rmse']:.6f}\")\n    print(f\"MAE:  {metrics['mae']:.6f}\")\n    print(f\"R\u00b2:   {metrics['r2']:.6f}\")\n    print(\"=\" * 50 + \"\\n\")\n</code></pre>"},{"location":"training/#model-pruning","title":"Model pruning","text":""},{"location":"training/#project_name.prune","title":"project_name.prune","text":""},{"location":"training/#project_name.prune.apply_unstructured_pruning","title":"apply_unstructured_pruning","text":"<pre><code>apply_unstructured_pruning(model: torch.nn.Module, amount: float) -&gt; dict[str, Any]\n</code></pre> <p>Apply unstructured L1 pruning to FC layers only, then make it permanent.</p> Source code in <code>src/project_name/prune.py</code> <pre><code>def apply_unstructured_pruning(\n    model: torch.nn.Module,\n    amount: float,\n) -&gt; dict[str, Any]:\n    \"\"\"Apply unstructured L1 pruning to FC layers only, then make it permanent.\"\"\"\n    if not (0.0 &lt;= amount &lt; 1.0):\n        raise ValueError(f\"Prune amount must be in [0, 1). Got: {amount}\")\n\n    pruned_modules = list(_iter_prunable_weight_params(model))\n    if not pruned_modules:\n        logger.warning(\"No prunable fully-connected (nn.Linear) layers found.\")\n        return {\"modules_pruned\": 0, \"global_sparsity\": 0.0}\n\n    for (m, pname) in pruned_modules:\n        prune.l1_unstructured(m, name=pname, amount=amount)\n\n    for (m, pname) in pruned_modules:\n        prune.remove(m, pname)\n\n    total_elems = 0\n    zero_elems = 0\n    for (m, pname) in pruned_modules:\n        w = getattr(m, pname)\n        total_elems += w.numel()\n        zero_elems += int((w == 0).sum().item())\n\n    global_sparsity = (zero_elems / total_elems) if total_elems &gt; 0 else 0.0\n    return {\n        \"modules_pruned\": len(pruned_modules),\n        \"global_sparsity\": global_sparsity,\n        \"zero_elems\": zero_elems,\n        \"total_elems\": total_elems,\n    }\n</code></pre>"},{"location":"training/#project_name.prune.evaluate_mse","title":"evaluate_mse","text":"<pre><code>evaluate_mse(model: torch.nn.Module, loader: DataLoader, device: torch.device, target_indices: Sequence[int]) -&gt; float\n</code></pre> <p>Mean MSE per graph (matches your train/eval convention).</p> Source code in <code>src/project_name/prune.py</code> <pre><code>@torch.no_grad()\ndef evaluate_mse(\n    model: torch.nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n    target_indices: Sequence[int],\n) -&gt; float:\n    \"\"\"Mean MSE per graph (matches your train/eval convention).\"\"\"\n    model.eval()\n\n    total_loss: float = 0.0\n    num_samples: int = 0\n    target_idx = list(target_indices)\n\n    for batch in loader:\n        batch = batch.to(device)\n        pred = model(batch)\n        target = batch.y[:, target_idx]\n        loss = torch.nn.functional.mse_loss(pred, target)\n        total_loss += float(loss.item()) * batch.num_graphs\n        num_samples += batch.num_graphs\n\n    return total_loss / max(1, num_samples)\n</code></pre>"},{"location":"training/#project_name.prune.measure_inference_latency","title":"measure_inference_latency","text":"<pre><code>measure_inference_latency(model: torch.nn.Module, loader: DataLoader, device: torch.device, *, warmup_batches: int = 10, timed_batches: int = 50) -&gt; dict[str, float]\n</code></pre> <p>Measures average latency per batch (ms) over a fixed number of batches.</p> <p>Notes: - Uses torch.inference_mode() via @torch.no_grad() + model.eval() - Syncs CUDA for accurate timing</p> Source code in <code>src/project_name/prune.py</code> <pre><code>@torch.no_grad()\ndef measure_inference_latency(\n    model: torch.nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n    *,\n    warmup_batches: int = 10,\n    timed_batches: int = 50,\n) -&gt; dict[str, float]:\n    \"\"\"\n    Measures average latency per batch (ms) over a fixed number of batches.\n\n    Notes:\n    - Uses torch.inference_mode() via @torch.no_grad() + model.eval()\n    - Syncs CUDA for accurate timing\n    \"\"\"\n    model.eval()\n\n    def _sync() -&gt; None:\n        if device.type == \"cuda\":\n            torch.cuda.synchronize()\n\n    it = iter(loader)\n\n    # Warmup\n    for _ in range(warmup_batches):\n        try:\n            batch = next(it)\n        except StopIteration:\n            it = iter(loader)\n            batch = next(it)\n        batch = batch.to(device)\n        _ = model(batch)\n    _sync()\n\n    # Timed\n    times: list[float] = []\n    for _ in range(timed_batches):\n        try:\n            batch = next(it)\n        except StopIteration:\n            it = iter(loader)\n            batch = next(it)\n        batch = batch.to(device)\n\n        _sync()\n        t0 = time.perf_counter()\n        _ = model(batch)\n        _sync()\n        t1 = time.perf_counter()\n        times.append(t1 - t0)\n\n    if not times:\n        return {\"ms_per_batch\": 0.0, \"batches\": 0}\n\n    avg_s = sum(times) / len(times)\n    return {\"ms_per_batch\": avg_s * 1000.0, \"batches\": float(len(times))}\n</code></pre>"},{"location":"training/#quantization","title":"Quantization","text":""},{"location":"training/#project_name.quantize","title":"project_name.quantize","text":""},{"location":"training/#project_name.quantize.measure_inference_latency","title":"measure_inference_latency","text":"<pre><code>measure_inference_latency(model: torch.nn.Module, loader: DataLoader, device: torch.device, *, warmup_batches: int = 10, timed_batches: int = 50) -&gt; dict[str, float]\n</code></pre> <p>Average latency per batch in ms. Note: for quantized CPU models this is the typical use case.</p> Source code in <code>src/project_name/quantize.py</code> <pre><code>@torch.no_grad()\ndef measure_inference_latency(\n    model: torch.nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n    *,\n    warmup_batches: int = 10,\n    timed_batches: int = 50,\n) -&gt; dict[str, float]:\n    \"\"\"\n    Average latency per batch in ms.\n    Note: for quantized CPU models this is the typical use case.\n    \"\"\"\n    model.eval()\n    it = iter(loader)\n\n    # Warmup\n    for _ in range(warmup_batches):\n        try:\n            batch = next(it)\n        except StopIteration:\n            it = iter(loader)\n            batch = next(it)\n        batch = batch.to(device)\n        _ = model(batch)\n\n    times: list[float] = []\n    for _ in range(timed_batches):\n        try:\n            batch = next(it)\n        except StopIteration:\n            it = iter(loader)\n            batch = next(it)\n\n        batch = batch.to(device)\n        t0 = time.perf_counter()\n        _ = model(batch)\n        t1 = time.perf_counter()\n        times.append(t1 - t0)\n\n    if not times:\n        return {\"ms_per_batch\": 0.0, \"batches\": 0.0}\n\n    avg_s = sum(times) / len(times)\n    return {\"ms_per_batch\": avg_s * 1000.0, \"batches\": float(len(times))}\n</code></pre>"},{"location":"training/#project_name.quantize.quantize_full_model","title":"quantize_full_model","text":"<pre><code>quantize_full_model(model: torch.nn.Module, scheme: str) -&gt; torch.nn.Module\n</code></pre> <p>Apply weight-only INT8 quantization to all linear layers in the model, including those nested inside GraphConv blocks. Uses torchao when available and falls back to the torch.ao dynamic quantization API otherwise.</p> scheme <ul> <li>\"torchao_int8_weight_only\" (default)</li> <li>\"torch_ao_dynamic\" (fallback-style dynamic quantization)</li> </ul> Source code in <code>src/project_name/quantize.py</code> <pre><code>def quantize_full_model(model: torch.nn.Module, scheme: str) -&gt; torch.nn.Module:\n    \"\"\"\n    Apply weight-only INT8 quantization to *all* linear layers in the model, including those\n    nested inside GraphConv blocks. Uses torchao when available and falls back to the\n    torch.ao dynamic quantization API otherwise.\n\n    scheme:\n      - \"torchao_int8_weight_only\" (default)\n      - \"torch_ao_dynamic\" (fallback-style dynamic quantization)\n    \"\"\"\n    if scheme == \"torch_ao_dynamic\":\n        from torch.ao.quantization import quantize_dynamic  # older weights-only API\n        return quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n\n    # default: torchao\n    try:\n        from torchao.quantization import quantize_\n        from torchao.quantization import Int8WeightOnlyConfig\n    except Exception as e:\n        logger.warning(\"torchao not available (%s). Falling back to torch.ao.quantization.quantize_dynamic.\", e)\n        from torch.ao.quantization import quantize_dynamic\n        return quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n\n    # torchao quantize_ is inplace; returns None or model depending on version\n    quantize_(model, Int8WeightOnlyConfig())  #  [oai_citation:3\u2021PyTorch Documentation](https://docs.pytorch.org/ao/stable/generated/torchao.quantization.quantize_.html)\n    return model\n</code></pre>"},{"location":"training/#promotion-and-comparison","title":"Promotion and comparison","text":""},{"location":"training/#project_name.compare_promote","title":"project_name.compare_promote","text":""},{"location":"training/#profiling","title":"Profiling","text":""},{"location":"training/#project_name.profiling","title":"project_name.profiling","text":"<p>Profiling utilities for training and evaluation.</p>"},{"location":"training/#project_name.profiling.TrainingProfiler","title":"TrainingProfiler","text":"<p>Manages profiling across entire training session.</p> Source code in <code>src/project_name/profiling.py</code> <pre><code>class TrainingProfiler:\n    \"\"\"Manages profiling across entire training session.\"\"\"\n\n    def __init__(\n        self,\n        enabled: bool = False,\n        output_dir: Optional[Path] = None,\n        warmup_steps: int = 1,\n        active_steps: int = 10,\n        repeat_steps: int = 1,\n    ) -&gt; None:\n        \"\"\"Initialize the training profiler.\n\n        Args:\n            enabled: Whether to enable profiling.\n            output_dir: Directory to save profiling results.\n        \"\"\"\n        self.enabled = enabled\n        self.output_dir = output_dir or Path(\"profiling_results/run\")\n        self.prof: Optional[profile] = None\n\n        if self.enabled:\n            self.output_dir.mkdir(parents=True, exist_ok=True)\n            self.prof = profile(\n                activities=[ProfilerActivity.CPU]\n                if not torch.cuda.is_available()\n                else [ProfilerActivity.CPU, ProfilerActivity.CUDA],\n                record_shapes=True,\n                profile_memory=True,\n                schedule=torch.profiler.schedule(\n                    wait=0,\n                    warmup=warmup_steps,\n                    active=active_steps,\n                    repeat=repeat_steps,\n                ),\n                on_trace_ready=tensorboard_trace_handler(output_dir),\n            )\n            self.prof.__enter__()\n\n    def step(self) -&gt; None:\n        \"\"\"Record a step (epoch) in the profiler.\"\"\"\n        if self.prof:\n            self.prof.step()\n\n    def finalize(self) -&gt; None:\n        \"\"\"Finalize profiling and export trace.\"\"\"\n        if self.prof:\n            self.prof.__exit__(None, None, None)\n            print(f\"\u2705 Profiling trace saved to {self.output_dir}\")\n</code></pre>"},{"location":"training/#project_name.profiling.TrainingProfiler.__init__","title":"__init__","text":"<pre><code>__init__(enabled: bool = False, output_dir: Optional[Path] = None, warmup_steps: int = 1, active_steps: int = 10, repeat_steps: int = 1) -&gt; None\n</code></pre> <p>Initialize the training profiler.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>Whether to enable profiling.</p> <code>False</code> <code>output_dir</code> <code>Optional[Path]</code> <p>Directory to save profiling results.</p> <code>None</code> Source code in <code>src/project_name/profiling.py</code> <pre><code>def __init__(\n    self,\n    enabled: bool = False,\n    output_dir: Optional[Path] = None,\n    warmup_steps: int = 1,\n    active_steps: int = 10,\n    repeat_steps: int = 1,\n) -&gt; None:\n    \"\"\"Initialize the training profiler.\n\n    Args:\n        enabled: Whether to enable profiling.\n        output_dir: Directory to save profiling results.\n    \"\"\"\n    self.enabled = enabled\n    self.output_dir = output_dir or Path(\"profiling_results/run\")\n    self.prof: Optional[profile] = None\n\n    if self.enabled:\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        self.prof = profile(\n            activities=[ProfilerActivity.CPU]\n            if not torch.cuda.is_available()\n            else [ProfilerActivity.CPU, ProfilerActivity.CUDA],\n            record_shapes=True,\n            profile_memory=True,\n            schedule=torch.profiler.schedule(\n                wait=0,\n                warmup=warmup_steps,\n                active=active_steps,\n                repeat=repeat_steps,\n            ),\n            on_trace_ready=tensorboard_trace_handler(output_dir),\n        )\n        self.prof.__enter__()\n</code></pre>"},{"location":"training/#project_name.profiling.TrainingProfiler.finalize","title":"finalize","text":"<pre><code>finalize() -&gt; None\n</code></pre> <p>Finalize profiling and export trace.</p> Source code in <code>src/project_name/profiling.py</code> <pre><code>def finalize(self) -&gt; None:\n    \"\"\"Finalize profiling and export trace.\"\"\"\n    if self.prof:\n        self.prof.__exit__(None, None, None)\n        print(f\"\u2705 Profiling trace saved to {self.output_dir}\")\n</code></pre>"},{"location":"training/#project_name.profiling.TrainingProfiler.step","title":"step","text":"<pre><code>step() -&gt; None\n</code></pre> <p>Record a step (epoch) in the profiler.</p> Source code in <code>src/project_name/profiling.py</code> <pre><code>def step(self) -&gt; None:\n    \"\"\"Record a step (epoch) in the profiler.\"\"\"\n    if self.prof:\n        self.prof.step()\n</code></pre>"},{"location":"training/#project_name.profiling.timing_checkpoint","title":"timing_checkpoint","text":"<pre><code>timing_checkpoint(name: str, enabled: bool = True) -&gt; Generator\n</code></pre> <p>Context manager for simple timing measurements.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name for this checkpoint.</p> required <code>enabled</code> <code>bool</code> <p>Whether to enable timing.</p> <code>True</code> <p>Yields:</p> Type Description <code>Generator</code> <p>Dictionary with timing results.</p> Source code in <code>src/project_name/profiling.py</code> <pre><code>@contextmanager\ndef timing_checkpoint(name: str, enabled: bool = True) -&gt; Generator:\n    \"\"\"Context manager for simple timing measurements.\n\n    Args:\n        name: Name for this checkpoint.\n        enabled: Whether to enable timing.\n\n    Yields:\n        Dictionary with timing results.\n    \"\"\"\n    result = {\"name\": name, \"duration\": 0.0}\n\n    if not enabled:\n        yield result\n        return\n\n    start = time.perf_counter()\n    try:\n        yield result\n    finally:\n        result[\"duration\"] = time.perf_counter() - start\n        print(f\"\u23f1\ufe0f  {name}: {result['duration']:.4f}s\")\n</code></pre>"},{"location":"utilities/","title":"Utilities and Visualization","text":"<p>Common helpers and visualization tools.</p>"},{"location":"utilities/#utilities","title":"Utilities","text":""},{"location":"utilities/#project_name.utils","title":"project_name.utils","text":"<p>Utility functions for data loading and environment detection.</p>"},{"location":"utilities/#project_name.utils.get_data_path","title":"get_data_path","text":"<pre><code>get_data_path(config_path: str | Path, gcs_bucket: str | None = None) -&gt; Path\n</code></pre> <p>Get the appropriate data path based on the environment.</p> <p>In cloud environments (GCP/Vertex AI), data is mounted to /gcs/. In local environments, data is loaded from the configured path. <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str | Path</code> <p>The data path from config (e.g., 'data' or 'data/processed').</p> required <code>gcs_bucket</code> <code>str | None</code> <p>Optional GCS bucket name. If provided and running in cloud,         will use /gcs/ as the base path. <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path object pointing to the correct data location.</p> Source code in <code>src/project_name/utils.py</code> <pre><code>def get_data_path(config_path: str | Path, gcs_bucket: str | None = None) -&gt; Path:\n    \"\"\"Get the appropriate data path based on the environment.\n\n    In cloud environments (GCP/Vertex AI), data is mounted to /gcs/&lt;bucket-name&gt;.\n    In local environments, data is loaded from the configured path.\n\n    Args:\n        config_path: The data path from config (e.g., 'data' or 'data/processed').\n        gcs_bucket: Optional GCS bucket name. If provided and running in cloud,\n                    will use /gcs/&lt;bucket-name&gt; as the base path.\n\n    Returns:\n        Path object pointing to the correct data location.\n    \"\"\"\n    gcs_mount = Path(\"/gcs\")\n\n    if gcs_mount.exists() and gcs_bucket:\n        data_path = gcs_mount / gcs_bucket / config_path\n    else:\n        data_path = Path(config_path)\n\n    return data_path\n</code></pre>"},{"location":"utilities/#visualization","title":"Visualization","text":""},{"location":"utilities/#project_name.visualize","title":"project_name.visualize","text":""}]}